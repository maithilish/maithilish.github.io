<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Scoopi Web Scraper V094 on Codetab</title>
    <link>http://www.codetab.org/tutorial/scoopi-web-scraper-v094/</link>
    <description>Recent content in Scoopi Web Scraper V094 on Codetab</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 12 Jan 2019 01:53:00 +0530</lastBuildDate>
    
	<atom:link href="http://www.codetab.org/tutorial/scoopi-web-scraper-v094/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Introduction - Scoopi Web Scraper V094 - CodeTab</title>
      <link>http://www.codetab.org/tutorial/scoopi-web-scraper-v094/introduction/</link>
      <pubDate>Sat, 12 Jan 2019 01:53:00 +0530</pubDate>
      
      <guid>http://www.codetab.org/tutorial/scoopi-web-scraper-v094/introduction/</guid>
      <description>Scoopi Web Scraper Scoopi web scraper extracts and transform data from HTML pages. JSoup and HtmlUnit makes it quite easy to scrape web pages in Java, but the things get complicated when data is from large number of pages. Some of the challenges while extracting large set of data from unstructured sources such as HTML pages are:
 Data being unstructured, may requires many queries to scrape them Data may not be in desired format and to make them usable, needs filter and transform Connection may drop during a run and all the work is lost When data is from thousands of pages, performance does matter Need Java or Python proficiency to use scraper libraries  Scraping libraries do well in scraping data from limited set of pages but they are not meant to handle thousands of pages.</description>
    </item>
    
    <item>
      <title>Installation - Scoopi Web Scraper - CodeTab</title>
      <link>http://www.codetab.org/tutorial/scoopi-web-scraper-v094/installation-quick-start-v094/</link>
      <pubDate>Sat, 12 Jan 2019 05:26:00 +0530</pubDate>
      
      <guid>http://www.codetab.org/tutorial/scoopi-web-scraper-v094/installation-quick-start-v094/</guid>
      <description>Scoopi Installation and Quick Start The easiest way to use Scoopi is to pull the docker image from DockerHub and run it straight away which comes with pre-configured MariaDB. In case, you are not using Docker then download the release from GitHub. We explain both the options here.
Install Scoopi from Docker Image Scoopi releases are available as docker image from DockerHub. To run the image you need Docker installed in the system and additionally, to run it with database, you also need Docker Compose.</description>
    </item>
    
    <item>
      <title>Definition File - Scoopi Web Scraper - CodeTab</title>
      <link>http://www.codetab.org/tutorial/scoopi-web-scraper-v094/definition-file-v094/</link>
      <pubDate>Sat, 12 Jan 2019 04:27:00 +0530</pubDate>
      
      <guid>http://www.codetab.org/tutorial/scoopi-web-scraper-v094/definition-file-v094/</guid>
      <description>Defs, Locators and Tasks Scoopi uses set of YML definition files to extract data from HTML pages. To learn the YML elements used by the definition files, Scoopi distribution comes with a set of examples which are under def/examples/jsoup folder. Examples are named as ex-1, ex-2 and so on, each with increasing complexity.
Scoopi Definition Files Scoopi creates the data model based on YML definition files. We can specify the definition file using scoopi.</description>
    </item>
    
    <item>
      <title>DataDef - Scoopi Web Scraper - CodeTab</title>
      <link>http://www.codetab.org/tutorial/scoopi-web-scraper-v094/datadef-v094/</link>
      <pubDate>Sat, 12 Jan 2019 02:47:00 +0530</pubDate>
      
      <guid>http://www.codetab.org/tutorial/scoopi-web-scraper-v094/datadef-v094/</guid>
      <description>DataDef Scoopi uses datadef to define data. Datadef contains axis, query, script and members which collectively defines the data to be scrapped from the HTML page.
In this chapter, we go through Example-1 job.xml to explain dataDef. This job.yml uses a simple DataDef which scrape one data point i.e. price of the company share from defs/examples/page/acme-quote.html page.
The datadef snippet from defs/examples/jsoup/ex-1/job.yml is as below
dataDefs: price: axis: fact: query: region: &amp;#34;div#price_tick&amp;#34; field: &amp;#34;*&amp;#34; col: query: script: configs.</description>
    </item>
    
    <item>
      <title>Selector - Scoopi Web Scraper - CodeTab</title>
      <link>http://www.codetab.org/tutorial/scoopi-web-scraper-v094/selector-v094/</link>
      <pubDate>Sat, 12 Jan 2019 04:34:00 +0530</pubDate>
      
      <guid>http://www.codetab.org/tutorial/scoopi-web-scraper-v094/selector-v094/</guid>
      <description>Query, Region and Field In this chapter we describe how to build query using JSoup Selectors.
Selectors In Example-1, we query price data from defs/examples/page/acme-quote.html page. The price snippet from this page is
&amp;lt;div id=&amp;#34;price_tick&amp;#34;&amp;gt; &amp;lt;span id=&amp;#34;price_tick_span&amp;#34;&amp;gt; &amp;lt;strong&amp;gt;315.25&amp;lt;/strong&amp;gt; &amp;lt;/span&amp;gt; &amp;lt;/div&amp;gt; To access the value the JSoup selector is
div#price_tick/*  Use Chrome to get Selector Chrome browser can assist you in constructing the Selector or XPath. To do that, open the HTML page in Chrome and select the item which we are interested.</description>
    </item>
    
    <item>
      <title>Members - Scoopi Web Scraper - CodeTab</title>
      <link>http://www.codetab.org/tutorial/scoopi-web-scraper-v094/members-v094/</link>
      <pubDate>Sat, 12 Jan 2019 03:40:00 +0530</pubDate>
      
      <guid>http://www.codetab.org/tutorial/scoopi-web-scraper-v094/members-v094/</guid>
      <description>Members This chapter explores extracting multiple values with members, match property and dynamic query.
The Example 2 extracts ten data points shown below from defs/examples/page/acme-quote.html page.
 MARKET CAP EPS (TTM) P/E P/C BOOK VALUE PRICE/BOOK DIV (%) DIV YIELD FACE VALUE INDUSTRY P/E  The snippet of HTML from the page is
&amp;lt;div id=&amp;#34;snapshot&amp;#34;&amp;gt; &amp;lt;div&amp;gt; &amp;lt;div&amp;gt; &amp;lt;div&amp;gt;MARKET CAP&amp;lt;/div&amp;gt; &amp;lt;div&amp;gt;382,642.57&amp;lt;/div&amp;gt; &amp;lt;div&amp;gt;&amp;lt;/div&amp;gt; &amp;lt;/div&amp;gt; &amp;lt;div&amp;gt; &amp;lt;div&amp;gt;P/E&amp;lt;/div&amp;gt; &amp;lt;div&amp;gt;-&amp;lt;/div&amp;gt; &amp;lt;div&amp;gt;&amp;lt;/div&amp;gt; &amp;lt;/div&amp;gt; &amp;lt;div&amp;gt; &amp;lt;div&amp;gt;BOOK VALUE&amp;lt;/div&amp;gt; &amp;lt;div&amp;gt;27.</description>
    </item>
    
    <item>
      <title>Index BreakAfter - Scoopi Web Scraper - CodeTab</title>
      <link>http://www.codetab.org/tutorial/scoopi-web-scraper-v094/index-indexrange-breakafter-v094/</link>
      <pubDate>Sat, 12 Jan 2019 03:23:00 +0530</pubDate>
      
      <guid>http://www.codetab.org/tutorial/scoopi-web-scraper-v094/index-indexrange-breakafter-v094/</guid>
      <description>Index, IndexRange and BreakAfter This chapter explains the use of index, indexRange and breakAfter to extract set of data without using multiple queries.
The Example 3 extracts multiple data from defs/examples/page/acme-bs.html page, which contains Balance Sheet data of Acme for past five years in a HTML table with 27 rows and 5 columns. The partial contents of the table is shown below.
   Item Dec &amp;lsquo;16 Dec &amp;lsquo;15 Dec &amp;lsquo;14 Dec &amp;lsquo;13 Dec &amp;lsquo;12     Total Share Capital 804.</description>
    </item>
    
    <item>
      <title>Filters - Scoopi Web Scraper - CodeTab</title>
      <link>http://www.codetab.org/tutorial/scoopi-web-scraper-v094/filters-v094/</link>
      <pubDate>Sat, 12 Jan 2019 03:12:00 +0530</pubDate>
      
      <guid>http://www.codetab.org/tutorial/scoopi-web-scraper-v094/filters-v094/</guid>
      <description>Filters The extracted data of the previous example contains many unwanted items and Scoopi provides filters to remove them from output.
The filter is normally applied after parse is over and members are created. The Example 5 uses filter for row axis. The filter snippet is as below
defs/examples/jsoup/ex-5/job.yml
row: query: region: &amp;#34;table:contains(Sources Of Funds)&amp;#34; field: &amp;#34;tr:nth-child(%{row.index}) &amp;gt; td:nth-child(1)&amp;#34; members: [ member: {name: item, index: 5, breakAfter: [&amp;#34;Book Value (Rs)&amp;#34;] }, ] filters: [ filter: { type: value, pattern: &amp;#34;&amp;#34; }, filter: { type: value, pattern: &amp;#34;Sources Of Funds&amp;#34; }, filter: { type: value, pattern: &amp;#34;Application Of Funds&amp;#34; }, ] The filter definition remove the members whose row axis value</description>
    </item>
    
    <item>
      <title>Multiple Tasks - Scoopi Web Scraper - CodeTab</title>
      <link>http://www.codetab.org/tutorial/scoopi-web-scraper-v094/multiple-tasks-v094/</link>
      <pubDate>Sat, 12 Jan 2019 03:54:00 +0530</pubDate>
      
      <guid>http://www.codetab.org/tutorial/scoopi-web-scraper-v094/multiple-tasks-v094/</guid>
      <description>Multiple Tasks Scoopi can execute multiple tasks for a locator and also multiple tasks on multiple locatorGroups.
Multiple tasks and single Locator group The Example-1 scrape Price data from acme-quote.html page while Example-2 extracts Snapshot data from the same page. One option is to define acme-quote.html locator in two locatorGroups and assign task to each of them. This unnecessarily downloads acme-quote.html twice. Instead, it is better to define single locatorGroup and assign two tasks - priceTask and snapshotTask so that page downloads only once.</description>
    </item>
    
    <item>
      <title>Steps - Scoopi Web Scraper - CodeTab</title>
      <link>http://www.codetab.org/tutorial/scoopi-web-scraper-v094/steps-and-plugins-v094/</link>
      <pubDate>Sat, 12 Jan 2019 05:01:00 +0530</pubDate>
      
      <guid>http://www.codetab.org/tutorial/scoopi-web-scraper-v094/steps-and-plugins-v094/</guid>
      <description>Steps So far, we explored locators to define the pages to scrape, tasks to execute and dataDef to parse data from the pages. But, we haven&amp;rsquo;t explained how Scoopi executes tasks and scrape data.
Scoopi is designed to execute tasks as workflow which is normally referred as steps which in turn consists of multiple step. By default, Scoopi ships with two defaults steps jsoupDefault and htmlUnitDefault and they are defined in steps-default.</description>
    </item>
    
    <item>
      <title>Converters - Scoopi Web Scraper - CodeTab</title>
      <link>http://www.codetab.org/tutorial/scoopi-web-scraper-v094/converters-v094/</link>
      <pubDate>Sat, 12 Jan 2019 02:25:00 +0530</pubDate>
      
      <guid>http://www.codetab.org/tutorial/scoopi-web-scraper-v094/converters-v094/</guid>
      <description>Converters This chapter explain step override and converters.
Step Override We can override any step at task level. The Example 8 overrides converter step of jsoupDefault steps in bs task. The relevant snippet is shown below
defs/examples/jsoup/ex-8/job.yml
bsGroup: bsTask: dataDef: bs steps: jsoupDefault: converter: class: &amp;#34;org.codetab.scoopi.step.convert.DataConverter&amp;#34; previous: process next: appender The other tasks in the example uses jsoupDefault steps as it is but the bsTask takes all steps from jsoupDefault and replace converter step with the step defined locally at task level.</description>
    </item>
    
    <item>
      <title>Link Locator - Scoopi Web Scraper - CodeTab</title>
      <link>http://www.codetab.org/tutorial/scoopi-web-scraper-v094/create-locators-links-v094/</link>
      <pubDate>Sat, 12 Jan 2019 05:16:00 +0530</pubDate>
      
      <guid>http://www.codetab.org/tutorial/scoopi-web-scraper-v094/create-locators-links-v094/</guid>
      <description>Create Locators from Links The definitions would become lengthy when we define each and every link in job.xml. Instead, we can use Scoopi to scrape links from a start page and dynamically create locators. This feature allows you to recursively scrape the web pages. Let&amp;rsquo;s see how to create locators from scraped links.
Link Scrape Step The Example 9 scrapes Balance Sheet and Profit &amp;amp; Loss links from acme-quote.html page.</description>
    </item>
    
    <item>
      <title>Persistence - Scoopi Web Scraper - CodeTab</title>
      <link>http://www.codetab.org/tutorial/scoopi-web-scraper-v094/persistence-v094/</link>
      <pubDate>Sat, 12 Jan 2019 04:11:00 +0530</pubDate>
      
      <guid>http://www.codetab.org/tutorial/scoopi-web-scraper-v094/persistence-v094/</guid>
      <description>Persistence With persistence, Scoopi offers following benefits.
 reduce network usage as it can reuse the downloaded pages recover from the aborted run without redoing the tasks already completed avoid expensive parse operation as it can reuse the persisted data set expiry date for each page  Setup Database If you running Scoopi from docker image then no database setup is required as docker image contains pre-configured MariaDB container. Please refer Scoopi with MariaDB to run Scoopi with MariaDB using docker-compose.</description>
    </item>
    
    <item>
      <title>Appender Encoder - Scoopi Web Scraper - CodeTab</title>
      <link>http://www.codetab.org/tutorial/scoopi-web-scraper-v094/appender-and-encoder-v094/</link>
      <pubDate>Sat, 12 Jan 2019 02:11:00 +0530</pubDate>
      
      <guid>http://www.codetab.org/tutorial/scoopi-web-scraper-v094/appender-and-encoder-v094/</guid>
      <description>Appender and Encoder Scoopi uses appender to append data to output and encoder to encode or alter the output format. At present, it comes with two appender - FileAppender and ListAppender.
FileAppender So far, all examples use FileAppender defined in default steps steps-default.yml packaged in scoopi distribution jar. The appender snippet from is as below
appender: class: &amp;#34;org.codetab.scoopi.step.load.DataAppender&amp;#34; previous: converter next: end plugins: [ plugin: { name: file, class: &amp;#34;org.</description>
    </item>
    
    <item>
      <title>Split Definitions - Scoopi Web Scraper - CodeTab</title>
      <link>http://www.codetab.org/tutorial/scoopi-web-scraper-v094/split-definitions-v094/</link>
      <pubDate>Sat, 12 Jan 2019 04:46:00 +0530</pubDate>
      
      <guid>http://www.codetab.org/tutorial/scoopi-web-scraper-v094/split-definitions-v094/</guid>
      <description>Split Definitions The Scoopi definition file tends to grow as you scrape more and more data sets or pages. This chapter explains how to split the definitions into multiple files for easy maintenance.
Scoopi allows you to split the definitions either on job types or on top level elements.
Split on Job Types The examples so far scraped BS, PL, Quote data - price and snapshot - of Acme. We can split the definitions into three job types quote, bs and pl.</description>
    </item>
    
    <item>
      <title>Logs - Scoopi Web Scraper - CodeTab</title>
      <link>http://www.codetab.org/tutorial/scoopi-web-scraper-v094/logs-v094/</link>
      <pubDate>Sat, 12 Jan 2019 03:34:00 +0530</pubDate>
      
      <guid>http://www.codetab.org/tutorial/scoopi-web-scraper-v094/logs-v094/</guid>
      <description>Logs Scoopi uses Logback library for logging. By default, it logs info message to console and log errors to logs/error.log file.
Debug and Trace logs The logback configuration file logback.xml is located in conf folder. By default root log level is info which outputs only error, warn and info logs. To output debug logs, change root level to debug and un-comment appender-ref ref=&amp;ldquo;debuglog&amp;rdquo; element.
Similarly for trace logs. Trace logs output query and parsed html nodes which is helpful to construct query strings.</description>
    </item>
    
    <item>
      <title>Dashboard - Scoopi Web Scraper - CodeTab</title>
      <link>http://www.codetab.org/tutorial/scoopi-web-scraper-v094/dashboard-v094/</link>
      <pubDate>Sat, 12 Jan 2019 02:35:00 +0530</pubDate>
      
      <guid>http://www.codetab.org/tutorial/scoopi-web-scraper-v094/dashboard-v094/</guid>
      <description>Dashboard Scoopi Dashboard is an nice little Angular web app that displays useful information such as system, task and pool stats.
The screenshot of the Scoopi dashboard is
 Embedded Jetty Web Server serves the dashboard and we can access the Scoopi dashboard at http://localhost:9010 while Scoopi is running. The dashboard can kept running even after scoopi is finished and use the same one for next run as it stops when scoopi terminates and refreshes itself once it detects that scoopi is running again.</description>
    </item>
    
    <item>
      <title>DB Setup - Scoopi Web Scraper - CodeTab</title>
      <link>http://www.codetab.org/tutorial/scoopi-web-scraper-v094/db-setup-v094/</link>
      <pubDate>Sat, 12 Jan 2019 03:02:00 +0530</pubDate>
      
      <guid>http://www.codetab.org/tutorial/scoopi-web-scraper-v094/db-setup-v094/</guid>
      <description>Setup HSQLDB Database for Scoopi To run examples with persistence we need HSQLDB up and running. Download HSQLDB version 2.3 and extract it to some folder. To create database and start HSQLDB server, run
cd hsqldb-2.3.0/hsqldb java -cp lib/hsqldb.jar org.hsqldb.server.Server --database.0 file:data/scoopi --dbname.0 scoopi  This creates database named scoopi and places database files in data directory.
Use HSQLDB client to connect running database and to do that run following from another console</description>
    </item>
    
  </channel>
</rss>